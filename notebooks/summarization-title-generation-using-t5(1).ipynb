{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os, psutil  \n\ndef cpu_stats():\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memory_use = py.memory_info()[0] / 2. ** 30\n    return 'memory GB:' + str(np.round(memory_use, 2))","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":3,"outputs":[{"output_type":"stream","text":"Sun Oct  4 06:04:02 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   40C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Download the preprocessed dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"cpu_stats()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'memory GB:0.14'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !wget https://www.dropbox.com/s/o3g9y88qgi8eapt/papers_eval_small.csv\n# !wget https://www.dropbox.com/s/pmw1jlmv8sy4gp5/papers_train_small.csv\n# # # !wget https://www.dropbox.com/s/slaa8uk2jlkq3wl/pytorch_model.bin","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install simpletransformers wandb pytorch-lightning","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nimport wandb\nimport random\nimport pandas as pd\nimport torch\nfrom simpletransformers.t5 import T5Model\n# from pytorch_lightning.metrics.nlp import BLEUScore\n","execution_count":11,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Without domain info\n**Simpletransformers implementation of T5 model expects a data to be a dataframe with 3 columns:**\n`<prefix>, <input_text>, <target_text>`\n* `<prefix>`: A string indicating the task to perform. (E.g. \"question\", \"stsb\")\n* `<input_text>`: The input text sequence (we will use Paper's abstract as `input_text`  )\n* `<target_text`: The target sequence (we will use Paper's title as `output_text` )\n    \n    \n You can read about the data format:  https://github.com/ThilinaRajapakse/simpletransformers#t5-transformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(include_domain=False):\n    \n    train_df = pd.read_csv(\"./papers_train_small.csv\")\n    eval_df = pd.read_csv(\"./papers_eval_small.csv\")\n    \n    train_df.dropna()\n    eval_df.dropna()\n  \n    # add domain tokens\n    if include_domain:\n        train_df.abstract = train_df.abstract + \" @domain: \" + train_df.categories\n        eval_df.abstract = eval_df.abstract + \" @domain: \" + eval_df.categories\n        \n    train_df = train_df[['title','abstract']]\n    eval_df = eval_df[['title','abstract']]\n    \n    train_df.columns = ['target_text', 'input_text']\n    eval_df.columns = ['target_text', 'input_text']\n    \n    \n    # task tokens\n    train_df['prefix'] = \"summarize\"\n    eval_df['prefix'] = \"summarize\"\n    \n    return train_df, eval_df\n\n    ","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df, eval_df = load_dataset(include_domain=False)","execution_count":40,"outputs":[{"output_type":"stream","text":"CPU times: user 1.02 s, sys: 83.7 ms, total: 1.1 s\nWall time: 1.1 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.input_text.iloc[0]","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"'  A rather non-standard quantum representation of the canonical commutation\\nrelations of quantum mechanics systems, known as the polymer representation has\\ngained some attention in recent years, due to its possible relation with Planck\\nscale physics. In particular, this approach has been followed in a symmetric\\nsector of loop quantum gravity known as loop quantum cosmology. Here we explore\\ndifferent aspects of the relation between the ordinary Schroedinger theory and\\nthe polymer description. The paper has two parts. In the first one, we derive\\nthe polymer quantum mechanics starting from the ordinary Schroedinger theory\\nand show that the polymer description arises as an appropriate limit. In the\\nsecond part we consider the continuum limit of this theory, namely, the reverse\\nprocess in which one starts from the discrete theory and tries to recover back\\nthe ordinary Schroedinger quantum mechanics. We consider several examples of\\ninterest, including the harmonic oscillator, the free particle and a simple\\ncosmological model.\\n'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape, eval_df.shape)","execution_count":42,"outputs":[{"output_type":"stream","text":"(100213, 3) (10520, 3)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Train without domain information"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_args = {\n    \"max_seq_length\": 512,\n    \"train_batch_size\": 8,\n    \"eval_batch_size\": 8,\n    \"num_train_epochs\": 5,\n    \"evaluate_during_training\": False,\n    \"evaluate_during_training_steps\": 1000,\n    \"evaluate_during_training_verbose\": True,\n    \n    \"use_multiprocessing\": False,\n    \"fp16\": False,\n\n    \"save_steps\": -1,\n    \"save_eval_checkpoints\": True,\n    \"save_model_every_epoch\": True,\n\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n\n    \"wandb_project\": \"title-generation\",\n    \n}\n","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create T5 Model\nmodel = T5Model(\"../input/trainedmodelwithout-domain/outputs/\", args=model_args, use_cuda=True)","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train T5 Model on new task\nmodel.train_model(train_data=train_df, eval_data=eval_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(\"t5-general.pth\", model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torch.load(\"t5-general.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_model(\"t5-general.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"hello\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torch.load(\"./pytorch_model.bin\")","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(model)","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"collections.OrderedDict"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate T5 Model on new task\nresults = model.eval_model(eval_df)\n\n# Predict with trained T5 model\n#print(model.predict([\"convert: four\"]))","execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10520.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b694312cb1c4c0aa8cf2dc6f5b662b9"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Running Evaluation'), FloatProgress(value=0.0, max=1315.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b655fd6c0af438db62b62ceab82fd01"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(results)","execution_count":46,"outputs":[{"output_type":"stream","text":"{'eval_loss': 1.8634847951026017}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## And We're Done ! \n**Let's see how our model performs in generating paper's titles**"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_num = 351\nactual_title = eval_df.iloc[random_num]['target_text']\nactual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\npredicted_title = model.predict(actual_abstract)\n\nprint(f'Actual Title: {actual_title}')\nprint(f'Predicted Title: {predicted_title}')\nprint(f'Actual Abstract: {actual_abstract}')\n","execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Generating outputs'), FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d8e9091be30428f8d223230352a0f3a"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Decoding outputs'), FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030646cfba08481c83127fb52ca7eebc"}},"metadata":{}},{"output_type":"stream","text":"\nActual Title: Hydrodynamics and beyond in the strongly coupled N=4 plasma\nPredicted Title: ['Hydrodynamic and higher quasinormal modes in AdS black hole background']\nActual Abstract: ['summarize:   We continue our investigations on the relation between hydrodynamic and\\nhigher quasinormal modes in the AdS black hole background started in\\narXiv:0710.4458 [hep-th]. As is well known, the quasinormal modes can be\\ninterpreted as the poles of the retarded Green functions of the dual N=4 gauge\\ntheory at finite temperature. The response to a generic perturbation is\\ndetermined by the residues of the poles. We compute these residues numerically\\nfor energy-momentum and R-charge correlators. We find that the diffusion modes\\nbehave in a similar way: at small wavelengths the residues go over into a form\\nof a damped oscillation and therefore these modes decouple at short distances.\\nThe sound mode behaves differently: its residue does not decay and at short\\nwavelengths this mode behaves as the higher quasinormal modes. Applications of\\nour findings include the definition of hydrodynamic length and time scales. We\\nalso show that the quasinormal modes, including the hydrodynamic diffusion\\nmodes, obey causality.\\n']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_num = 777\nactual_title = eval_df.iloc[random_num]['target_text']\nactual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\npredicted_title = model.predict(actual_abstract)\nprint(f'Actual Title: {actual_title}')\nprint(f'Predicted Title: {predicted_title}')\nprint(f'Actual Abstract: {actual_abstract}')","execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Generating outputs'), FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c852efa9e046709d3ff5ca6c278a4d"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Decoding outputs'), FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a254eeab5bb436ba2697695485c17f1"}},"metadata":{}},{"output_type":"stream","text":"\nActual Title: Towards a homotopy theory of process algebra\nPredicted Title: ['labelled flows: a homotopical approach to higher dimensional automata']\nActual Abstract: ['summarize:   This paper proves that labelled flows are expressive enough to contain all\\nprocess algebras which are a standard model for concurrency. More precisely, we\\nconstruct the space of execution paths and of higher dimensional homotopies\\nbetween them for every process name of every process algebra with any\\nsynchronization algebra using a notion of labelled flow. This interpretation of\\nprocess algebra satisfies the paradigm of higher dimensional automata (HDA):\\none non-degenerate full $n$-dimensional cube (no more no less) in the\\nunderlying space of the time flow corresponding to the concurrent execution of\\n$n$ actions. This result will enable us in future papers to develop a\\nhomotopical approach of process algebras. Indeed, several homological\\nconstructions related to the causal structure of time flow are possible only in\\nthe framework of flows.\\n']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_num = 187\nactual_title = eval_df.iloc[random_num]['target_text']\nactual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\npredicted_title = model.predict(actual_abstract)\n\nprint(f'Actual Title: {actual_title}')\nprint(f'Predicted Title: {predicted_title}')\nprint(f'Actual Abstract: {actual_abstract}')","execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Generating outputs'), FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c2dbe2dc96a4ef7ad8cc67f9d055d26"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Decoding outputs'), FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4db389575234452bc984f5bbc4e82a0"}},"metadata":{}},{"output_type":"stream","text":"\nActual Title: D=5 M-theory radion supermultiplet dynamics\nPredicted Title: ['Radion Supermultiplet and the Cosmological Model']\nActual Abstract: ['summarize:   We show how the bosonic sector of the radion supermultiplet plus d=4, N=1\\nsupergravity emerge from a consistent braneworld Kaluza-Klein reduction of D=5\\nM--theory. The radion and its associated pseudoscalar form an SL(2,R)/U(1)\\nnonlinear sigma model. This braneworld system admits its own brane solution in\\nthe form of a 2-supercharge supersymmetric string. Requiring this to be free of\\nsingularities leads to an SL(2,Z) identification of the sigma model target\\nspace. The resulting radion mode has a minimum length; we suggest that this\\ncould be used to avoid the occurrence of singularities in brane-brane\\ncollisions. We discuss possible supersymmetric potentials for the radion\\nsupermultiplet and their relation to cosmological models such as the cyclic\\nuniverse or hybrid inflation.\\n']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Predict the abstract for whole test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_titles  =  model.predict(\"summarize: \"+eval_df.input_text)","execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Generating outputs'), FloatProgress(value=0.0, max=1315.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4deac52ed79e479eb96527e598dbd3a7"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Decoding outputs'), FloatProgress(value=0.0, max=10520.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c85e5581e06412cb1be3fd230b943e0"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(predicted_titles), len(predicted_titles), eval_df.shape)","execution_count":53,"outputs":[{"output_type":"stream","text":"<class 'list'> 10520 (10520, 3)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df[\"predicted_titles\"] = predicted_titles","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df.to_csv(\"eval_df_preds_without_domain.csv\", index=False)","execution_count":55,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Domain Controlled"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df, eval_df = load_dataset(include_domain=True)","execution_count":15,"outputs":[{"output_type":"stream","text":"CPU times: user 950 ms, sys: 181 ms, total: 1.13 s\nWall time: 1.13 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_args = {\n    \"max_seq_length\": 512,\n    \"train_batch_size\": 8,\n    \"eval_batch_size\": 8,\n    \"num_train_epochs\": 5,\n    \"evaluate_during_training\": False,\n    \"evaluate_during_training_steps\": 1000,\n    \"evaluate_during_training_verbose\": True,\n    \n    \"use_multiprocessing\": False,\n    \"fp16\": False,\n\n    \"save_steps\": -1,\n    \"save_eval_checkpoints\": True,\n    \"save_model_every_epoch\": True,\n\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n\n    \"wandb_project\": \"title-generation\",\n    \n}\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create T5 Model\nmodel = T5Model(\"t5-small\", args=model_args, use_cuda=True)","execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0b21d5a404044ee8cd4895674deacdb"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93dc7144144f443e8c587873c47a5e90"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24aa8a912600477b89be9af90d154720"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train T5 Model on new task\nmodel.train_model(train_data=train_df, eval_data=eval_df)","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=100213.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ff7ad514414459e97b6ce3ba0ad8be1"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b4898d4929e4999b06d2a4b1b5b65fc"}},"metadata":{}},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","name":"stderr"},{"output_type":"stream","name":"stdout","text":"wandb: Paste an API key from your profile and hit enter: ········\n"},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmacab\u001b[0m (use `wandb login --relogin` to force relogin)\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.4 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201004_061335-38h1907u\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfresh-vortex-14\u001b[0m\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/macab/title-generation\" target=\"_blank\">https://wandb.ai/macab/title-generation</a><br/>\n                Run page: <a href=\"https://wandb.ai/macab/title-generation/runs/38h1907u\" target=\"_blank\">https://wandb.ai/macab/title-generation/runs/38h1907u</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 5', max=12527.0, style=ProgressStyle(d…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e21bdcb430e48b8950f44d38f0945e9"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1 of 5', max=12527.0, style=ProgressStyle(d…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56e46e9ca41a4201af74a6aed036e89f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2 of 5', max=12527.0, style=ProgressStyle(d…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b95a946be724ddd9a40ad291b2a9806"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3 of 5', max=12527.0, style=ProgressStyle(d…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6a26e3221f14c13aabd748896b9fcc8"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4 of 5', max=12527.0, style=ProgressStyle(d…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bdf68a2a51a4344ababf340fb84606e"}},"metadata":{}},{"output_type":"stream","text":"\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"continue\")","execution_count":19,"outputs":[{"output_type":"stream","text":"continue\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate T5 Model on new task\nresults = model.eval_model(eval_df)\n\n# Predict with trained T5 model\n#print(model.predict([\"convert: four\"]))","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=10520.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fd5903fc6c9406f8e43d78c16ab4dd7"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=1315.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3bb609661894ea784341b5aa6819461"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(results)","execution_count":21,"outputs":[{"output_type":"stream","text":"{'eval_loss': 1.8594654024327208}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_num = 351\nactual_title = eval_df.iloc[random_num]['target_text']\nactual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\npredicted_title = model.predict(actual_abstract)\n\nprint(f'Actual Title: {actual_title}')\nprint(f'Predicted Title: {predicted_title}')\nprint(f'Actual Abstract: {actual_abstract}')\n8594654024327208}","execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Generating outputs', max=1.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715a32f79ebb44e5b31e924d99dd3140"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57e680> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Decoding outputs', max=1.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"503da4f0b3064aee90caa9e1b4d333cf"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f55d4d0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f55d710> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f55d830> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f55db00> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f55dcb0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f55de60> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57b050> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57b200> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57b3b0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57b560> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57b710> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57b8c0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57ba70> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57bc20> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57bdd0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57bf80> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57d170> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57d320> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57d4d0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57d680> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57d830> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57d9e0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57db90> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57dd40> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f57def0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5600e0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f560290> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f560440> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5605f0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5607a0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f560950> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f560b00> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f560cb0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f560e60> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54d050> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54d200> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54d3b0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54d560> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54d710> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54d8c0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54da70> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54dc20> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54ddd0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54df80> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f552170> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f552320> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5524d0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f552680> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f552830> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5529e0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f552b90> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f552d40> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f552ef0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f541170> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5413b0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f541710> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5419e0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5414d0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f541050> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f586dec0b90> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f541a70> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f541e60> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f541c20> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f548f80> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f548170> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5483b0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f548560> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f548710> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5489e0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f548c20> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f548ef0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f548d40> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54a050> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54a200> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54a3b0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54a560> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54a710> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54a8c0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54aa70> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54ac20> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54add0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54af80> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f545170> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f545320> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5454d0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f545680> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f545830> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5459e0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f545b90> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f545d40> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f545ef0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54c0e0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54c290> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n","name":"stderr"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54c440> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54c5f0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54c7a0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54c950> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54cb00> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54ccb0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f54ce60> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f547050> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f547200> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5473b0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f547560> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f547710> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f5478c0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f547a70> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f547c20> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f547dd0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583f547f80> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d555170> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d555320> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d5554d0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d555680> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d555830> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d5559e0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d555b90> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d555d40> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d555ef0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d55a0e0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d55a290> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d55a440> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d55a5f0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d55a7a0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d55a950> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d55ab00> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d55acb0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d55ae60> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d546050> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d546200> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n/opt/conda/lib/python3.7/site-packages/torch/utils/hooks.py:59: UserWarning: backward hook <function TorchHistory._hook_variable_gradient_stats.<locals>.<lambda> at 0x7f583d5463b0> on tensor will not be serialized.  If this is expected, you can decorate the function with @torch.utils.hooks.unserializable_hook to suppress this warning\n  \"to suppress this warning\".format(repr(hook)))\n","name":"stderr"},{"output_type":"stream","text":"\nActual Title: Hydrodynamics and beyond in the strongly coupled N=4 plasma\nPredicted Title: ['Hydrodynamic and higher quasinormal modes in AdS black hole background']\nActual Abstract: ['summarize:   We continue our investigations on the relation between hydrodynamic and\\nhigher quasinormal modes in the AdS black hole background started in\\narXiv:0710.4458 [hep-th]. As is well known, the quasinormal modes can be\\ninterpreted as the poles of the retarded Green functions of the dual N=4 gauge\\ntheory at finite temperature. The response to a generic perturbation is\\ndetermined by the residues of the poles. We compute these residues numerically\\nfor energy-momentum and R-charge correlators. We find that the diffusion modes\\nbehave in a similar way: at small wavelengths the residues go over into a form\\nof a damped oscillation and therefore these modes decouple at short distances.\\nThe sound mode behaves differently: its residue does not decay and at short\\nwavelengths this mode behaves as the higher quasinormal modes. Applications of\\nour findings include the definition of hydrodynamic length and time scales. We\\nalso show that the quasinormal modes, including the hydrodynamic diffusion\\nmodes, obey causality.\\n @domain: hep-th']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_num = 777\nactual_title = eval_df.iloc[random_num]['target_text']\nactual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\npredicted_title = model.predict(actual_abstract)\nprint(f'Actual Title: {actual_title}')\nprint(f'Predicted Title: {predicted_title}')\nprint(f'Actual Abstract: {actual_abstract}')","execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Generating outputs', max=1.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a45ca94a5f43eea62b7722d4121959"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Decoding outputs', max=1.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9b413cd436540539aff14d8f8720831"}},"metadata":{}},{"output_type":"stream","text":"\nActual Title: Towards a homotopy theory of process algebra\nPredicted Title: ['labelled flows and homotopical approach to synchronization algebras']\nActual Abstract: ['summarize:   This paper proves that labelled flows are expressive enough to contain all\\nprocess algebras which are a standard model for concurrency. More precisely, we\\nconstruct the space of execution paths and of higher dimensional homotopies\\nbetween them for every process name of every process algebra with any\\nsynchronization algebra using a notion of labelled flow. This interpretation of\\nprocess algebra satisfies the paradigm of higher dimensional automata (HDA):\\none non-degenerate full $n$-dimensional cube (no more no less) in the\\nunderlying space of the time flow corresponding to the concurrent execution of\\n$n$ actions. This result will enable us in future papers to develop a\\nhomotopical approach of process algebras. Indeed, several homological\\nconstructions related to the causal structure of time flow are possible only in\\nthe framework of flows.\\n @domain: math.AT math.CT']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_num = 187\nactual_title = eval_df.iloc[random_num]['target_text']\nactual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\npredicted_title = model.predict(actual_abstract)\n\nprint(f'Actual Title: {actual_title}')\nprint(f'Predicted Title: {predicted_title}')\nprint(f'Actual Abstract: {actual_abstract}')","execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Generating outputs', max=1.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a4762379674e9185b2487a2a469fcc"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Decoding outputs', max=1.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f551d9237f740e98392a10f14db6635"}},"metadata":{}},{"output_type":"stream","text":"\nActual Title: D=5 M-theory radion supermultiplet dynamics\nPredicted Title: ['Radion Supermultiplet and the Cosmological Model']\nActual Abstract: ['summarize:   We show how the bosonic sector of the radion supermultiplet plus d=4, N=1\\nsupergravity emerge from a consistent braneworld Kaluza-Klein reduction of D=5\\nM--theory. The radion and its associated pseudoscalar form an SL(2,R)/U(1)\\nnonlinear sigma model. This braneworld system admits its own brane solution in\\nthe form of a 2-supercharge supersymmetric string. Requiring this to be free of\\nsingularities leads to an SL(2,Z) identification of the sigma model target\\nspace. The resulting radion mode has a minimum length; we suggest that this\\ncould be used to avoid the occurrence of singularities in brane-brane\\ncollisions. We discuss possible supersymmetric potentials for the radion\\nsupermultiplet and their relation to cosmological models such as the cyclic\\nuniverse or hybrid inflation.\\n @domain: hep-th astro-ph']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_titles  =  model.predict(\"summarize: \"+eval_df.input_text)","execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Generating outputs', max=1315.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbfd82dfb38e46ca8ff808b6fa34047e"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Decoding outputs', max=10520.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"705bced6def948f58af03e78c93cd4f1"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_titles[0]","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"'Semi-structured interviews with smart home owners'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df.to_csv(\"eval_df_preds_with_domain.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}