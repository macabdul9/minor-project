{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "## kaggle \n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        \n",
    "    \n",
    "\n",
    "import os, psutil  \n",
    "\n",
    "def cpu_stats():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memory_use = py.memory_info()[0] / 2. ** 30\n",
    "    return 'memory GB:' + str(np.round(memory_use, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Guidelines to follow before working on this**\n",
    "- Comment your code well\n",
    "- See **to do** and update after doing something (at the end)\n",
    "- Write readable code\n",
    "- Save stats after performing the experiment (training)\n",
    "- Hyperparameters must be same throughout the whole training/experiment.\n",
    "\n",
    "## Done\n",
    "Dataset is ready to feed to pytorch models.\n",
    "\n",
    "## **To Do**\n",
    "- Create Recurrent Seq2Seq model(baseline) \n",
    "- Training/Testing with metrics like ppl and blue score (not limited to), one can use other metrics as well but these two must be there. Also don't forget to add loggers and checkpoints (Tensorboard, EarlyStopping and ModelCheckpoint are must rest are optional)\n",
    "- Start from **Run from here after import**\n",
    "- Update the below **Results** table\n",
    "\n",
    "\n",
    "\n",
    "## **Result**\n",
    "\n",
    "|   Model Name  |    Train_Loss |      Train_PPL |    Train_BLEU |       Val_Loss |       Val_PPL | Val_BLEU     |\n",
    "| ------------- | ------------- |  ------------- | ------------- |  ------------- | ------------- |------------- |\n",
    "| Content Cell  | Content Cell  |  Content Cell  | Content Cell  |  Content Cell  | Content Cell  |Content Cell  |\n",
    "| Content Cell  | Content Cell  |  Content Cell  | Content Cell  |  Content Cell  | Content Cell  |Content Cell  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, utils, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'memory GB:0.22'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./dataset/arxiv-metadata-oai-snapshot.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Calculation of prompt diphoton production cross sections at Tevatron and\n",
      "  LHC energies\n",
      "\n",
      " Abstract:   A fully differential calculation in perturbative quantum chromodynamics is\n",
      "presented for the production of massive photon pairs at hadron colliders. All\n",
      "next-to-leading order perturbative contributions from quark-antiquark,\n",
      "gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\n",
      "all-orders resummation of initial-state gluon radiation valid at\n",
      "next-to-next-to-leading logarithmic accuracy. The region of phase space is\n",
      "specified in which the calculation is most reliable. Good agreement is\n",
      "demonstrated with data from the Fermilab Tevatron, and predictions are made for\n",
      "more detailed tests with CDF and DO data. Predictions are shown for\n",
      "distributions of diphoton pairs produced at the energy of the Large Hadron\n",
      "Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\n",
      "boson are contrasted with those produced from QCD processes at the LHC, showing\n",
      "that enhanced sensitivity to the signal can be obtained with judicious\n",
      "selection of events.\n",
      "\n",
      "\n",
      " Ref: Phys.Rev.D76:013009,2007\n",
      "\n",
      " Cat: hep-ph\n"
     ]
    }
   ],
   "source": [
    "metadata = get_metadata()\n",
    "for paper in metadata:\n",
    "    paper_dict = json.loads(paper)\n",
    "    print(f'Title: {paper_dict.get(\"title\")}\\n\\n Abstract: {paper_dict.get(\"abstract\")}\\n\\n Ref: {paper_dict.get(\"journal-ref\")}\\n\\n Cat: {paper_dict.get(\"categories\")[0]}')\n",
    "#     print(paper)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Papers abstract, title and other infos from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274255 274255 274255 274255\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "abstracts = []\n",
    "years = []\n",
    "cats = []\n",
    "\n",
    "metadata = get_metadata()\n",
    "for paper in metadata:\n",
    "    paper_dict = json.loads(paper)\n",
    "    ref = paper_dict.get('journal-ref')\n",
    "    try:\n",
    "        year = int(ref[-4:])\n",
    "        if 2021 > year > 2000:\n",
    "            years.append(year)\n",
    "            titles.append(paper_dict.get('title'))\n",
    "            abstracts.append(paper_dict.get('abstract'))\n",
    "            cats.append(paper_dict.get(\"categories\"))\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "print(len(titles), len(abstracts), len(years), len(cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.DataFrame({\n",
    "    \"title\":titles,\n",
    "    \"abstract\":abstracts,\n",
    "    \"year\":years,\n",
    "    \"categories\":cats\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>2007</td>\n",
       "      <td>[hep-ph]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polymer Quantum Mechanics and its Continuum Limit</td>\n",
       "      <td>A rather non-standard quantum representation...</td>\n",
       "      <td>2007</td>\n",
       "      <td>[gr-qc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Spitzer c2d Survey of Large, Nearby, Inste...</td>\n",
       "      <td>We discuss the results from the combined IRA...</td>\n",
       "      <td>2007</td>\n",
       "      <td>[astro-ph]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fermionic superstring loop amplitudes in the p...</td>\n",
       "      <td>The pure spinor formulation of the ten-dimen...</td>\n",
       "      <td>2007</td>\n",
       "      <td>[hep-th]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lifetime of doubly charmed baryons</td>\n",
       "      <td>In this work, we evaluate the lifetimes of t...</td>\n",
       "      <td>2008</td>\n",
       "      <td>[hep-ph]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1  Polymer Quantum Mechanics and its Continuum Limit   \n",
       "2  The Spitzer c2d Survey of Large, Nearby, Inste...   \n",
       "3  Fermionic superstring loop amplitudes in the p...   \n",
       "4                 Lifetime of doubly charmed baryons   \n",
       "\n",
       "                                            abstract  year  categories  \n",
       "0    A fully differential calculation in perturba...  2007    [hep-ph]  \n",
       "1    A rather non-standard quantum representation...  2007     [gr-qc]  \n",
       "2    We discuss the results from the combined IRA...  2007  [astro-ph]  \n",
       "3    The pure spinor formulation of the ten-dimen...  2007    [hep-th]  \n",
       "4    In this work, we evaluate the lifetimes of t...  2008    [hep-ph]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the dataframe\n",
    "papers.to_csv(\"papers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from here after import\n",
    "- [Download Preprocessed Data](https://www.dropbox.com/s/ta5z9ec3rc8bju8/preprocessed.zip?dl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text fields, since both title and abstract are text only single can be used for both\n",
    "field = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"en\",\n",
    "    batch_first=True\n",
    ")\n",
    "fields = [(\"title\", field), (\"abstract\", field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Don't do it if you're loading pre-processed data, read the data\n",
    "# dataset = TabularDataset(path=\"papers.csv\", format=\"CSV\", fields=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data so that next time we need not to preprocess (that's a hack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the data so that next time we don't need preprocessing\n",
    "# with open(\"field.pkl\", \"wb\") as fp:\n",
    "#     dill.dump(field, fp)\n",
    "    \n",
    "# with open(\"data.pkl\", \"wb\") as fp:\n",
    "#     dill.dump(dataset.examples, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and \n",
    "with open(\"data.pkl\", 'rb') as fp:\n",
    "    examples = dill.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset  = Dataset(examples=examples, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-val split\n",
    "train, val = dataset.split(split_ratio=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "field.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264898"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size\n",
    "len(field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation iterator\n",
    "BATCH_SIZE = 64\n",
    "train_loader, val_loader = BucketIterator.splits(\n",
    "    datasets=(train, val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    sort=False,\n",
    "#     device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28]) torch.Size([64, 372])\n"
     ]
    }
   ],
   "source": [
    "print(batch.title.shape, batch.abstract.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- See to do at the top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "# import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(field.vocab)\n",
    "embedding_dim = 256\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    \"\"\"\n",
    "        encoder module will encode the source text (abstract in this case) \n",
    "        into a(or many if it is multi-layer) contextualized tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(SequentialModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size,\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # embedding layer to embed the tokens\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # reccurent layer whose last hidden state will be context of whole sequence\n",
    "        self.gru = nn.GRU(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout=0.15):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        #encoder model to encoder the abstract\n",
    "        self.encoder = SequentialModel(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_size=hidden_size)\n",
    "        \n",
    "        # decoder is a LM\n",
    "        self.decoder = SequentialModel(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_size=hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc_out = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "            src.shape -> [batch, src_len]\n",
    "            trg.shape -> [batch, trg_len]\n",
    "        \"\"\"\n",
    "        _, hidden = self.encoder(src)\n",
    "        outputs, _ = self.decoder(trg, hidden)\n",
    "        prediction = self.fc_out(self.dropout(outputs))\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Seq2Seq(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_size=hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer and criterion\n",
    "PAD_IDX = field.vocab.stoi[field.pad_token]\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data_loader, crtierion):\n",
    "    losses = []\n",
    "    ppl = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            abstract, title = batch.abstract, batch.title\n",
    "            batch_size, title_len = title.shape[0], title.shape[1]\n",
    "            outputs = model(abstract.to(device), title.to(device))\n",
    "            l = criterion(outputs.view(batch_size*title_len, -1), title.view(-1))\n",
    "            losses.append(l.item())\n",
    "            ppl.append(torch.exp(l).item())\n",
    "            \n",
    "    return sum(losses)/len(losses), sum(ppl)/len(ppl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, criterion, optimizer, EPOCHS=10):\n",
    "    total_steps = len(data_loader)*EPOCHS\n",
    "    steps = 0\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        losses = []\n",
    "        ppl = []\n",
    "        step_progress = tqdm(total=len(train_loader), desc=\"Step\", position=0)\n",
    "        for batch in tqdm(data_loader):\n",
    "            abstract, title = batch.abstract.to(device), batch.title.to(device)\n",
    "            batch_size, title_len = title.shape[0], title.shape[1]\n",
    "            outputs = model(abstract, title)\n",
    "            \n",
    "            loss = criterion(outputs.view(batch_size*title_len, -1), title.view(-1))\n",
    "            \n",
    "            # backpropgagte the loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            ppl.append(torch.exp(loss).item())\n",
    "            \n",
    "            if steps%1000==0:\n",
    "                print(f'Steps {steps}/{total_steps} | Train_loss {loss.item():.3f} | Train_ppl {torch.exp(loss).item():.3f}')\n",
    "            \n",
    "            steps += 1\n",
    "            step_progress.update(1)\n",
    "        \n",
    "        avg_loss = sum(losses)/len(losses)\n",
    "        avg_ppl = sum(ppl)/len(ppl)\n",
    "        \n",
    "        val_loss, val_ppl = eval(model, val_loader, criterion)\n",
    "        \n",
    "        print(f'Epoch {epoch}/{EPOCHS} | Steps {steps}/{total_steps}\\nTrain_loss {avg_loss:.3f} | Train_ppl {avg_ppl:.3f}\\nVal_loss {val_loss:.3f} Val_ppl {val_ppl:.3f}') \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
